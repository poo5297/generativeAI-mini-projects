{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLA8PJP5OTsw"
      },
      "source": [
        "In this notbook, we explore how to apply LoRA, a technique for Parameter Efficient Fine-tuning (PEFT) and then evaluate the model’s performance before and after fine-tuning to highlight the improvements. PEFT is a type of instruction fine-tuning that caters to specific LLM tasks, making the model tuned to a specific task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr2PNtstbuJ8"
      },
      "source": [
        "Running with **T4 GPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFpOymOdQ6x0",
        "outputId": "e80d0228-9e53-48f7-8f5f-185cb2586d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch                                  # Pytorch\n",
        "!pip install -q transformers datasets                  # Comes from HuggingFace\n",
        "!pip install -q bitsandbytes                           # For quantization from HuggingFace\n",
        "!pip install -q peft                                   # Parameter-efficient Fine-tuning from HuggingFace\n",
        "!pip install -q trl                                    # For supervised fine-tuning for LLMs from HuggingFace\n",
        "!pip install -q accelerate                             # For distributed training from HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jZQgqNxcBxh"
      },
      "source": [
        "**1. Loading Mistral Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vrz_fWR6UaLP"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "HF_API_TOKEN = \"insert token\"\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = HF_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370,
          "referenced_widgets": [
            "eed9683eeaac4417a3870f3e0a485c52",
            "81b6922eee7644c49c74dc43948d8fba",
            "45ea5cbf02d040d6aac226eebc0eed5e",
            "ae7abbf2f6cd4d03923e1f2344598c60",
            "538c2d2ee5d94f3e8addd1b5ab9ff3a4",
            "e4b7d8a43f5c46f2882682425a7271d0",
            "4461698fd3734ae6bd66c72a790ab850",
            "8a8bcdf3fb5e4d7087b077c975841326",
            "142b5ac6dd8b4c3c9193f440b8c3a6c4",
            "6d73130da6d94d408197ccc1c15f6756",
            "8c3afa75476e4c6695a9a9d88535801d",
            "940d9519138b43c1bef2bdbab6fe7321",
            "9016b685f12d4dfe89cf7bafbe216d33",
            "2e0beddefd044734b0d78f58384b0bbc",
            "4d412529340a4dfcb41cdeefd998c12f",
            "d1696a29b453421381feeb143e011592",
            "24168fe130534335a1c7902d984372c5",
            "2c1f3d0c47014c7d914dd7a505f56eb8",
            "988798f7f7f04fd3b66eb0f3f0760246",
            "6377c86b774d4b9e95d0b865db774b88",
            "532edfd429fc421f89795adc4a65ffb2",
            "7273d367b2f3486eb177d5a3cfcdc46a",
            "011dab0ba2fd4842b3dfae0789352807",
            "9a4ee18119634b35bca2bcde67664d33",
            "b539ba150a2c4205a1f5fa4f96bdcf42",
            "d4bfb30658ed4ef08f6e8e464b27aad3",
            "73aafd33596e49df92260f69972edabd",
            "ad4dcb16d9284a4bad38fc71fda1285e",
            "a7a8d82143954ceb886a7729dd514814",
            "7441abe4ea8a433ca14b0d71ebd28f40",
            "169aa6031a2f48debecf337aad62713f",
            "a68992f2850f44ed8320937b76965eca",
            "d3cc451d67984f2081267f1531b53c35",
            "bc7da3c2fb8c4061ae39d02e156fdc9c",
            "70fc8adecf204f1bbffdf41d91e43c8f",
            "89e9f53bca7448f182c019adefda757e",
            "25de4f04efa6440eb26925944bd7f04a",
            "f38941da6eb14b288f84671bca0274e9",
            "95ea6579b705449d87775a8efcc87ded",
            "207a00a37bc84ac88cbd89abba0f51a7",
            "aa250d0faab54f3d97a6a3ea5b437ee9",
            "0d1d33ca550742a59d009250f96a2a06",
            "4d2b6900df0547efa7b58ecd92d292dc",
            "85e8ec4621434dd8bca85835e0b5a7d4",
            "7259f5a87d9d40f9a805965b32c227a3",
            "0fec9daad80b4d88a1470fb733b35e83",
            "b45041bb2dca433eb185dca53da4e235",
            "1c5e3f20476d427bb4b648f71b042a84",
            "1b68f2e7645647358b22e5b342c22b9d",
            "de7b5e2b21d54e129bd2456936daf832",
            "b1dfe1d36ca44455affedc658e028922",
            "3687accd2a924a90a012765281ce160d",
            "5eddc5ac9c5342259ac4d8c1b1e757dc",
            "e72cc1fc1ef8450e9847718a5efe5b9f",
            "45900a675c204296805308dd05c9e5b5",
            "f986285831a94b8dac24565aee14521a",
            "7028878d6f734641ad3f2487ff391fb2",
            "fcfacba654874295b2e088c32504c3e2",
            "20da1cf96a65421880454e27c84d05c4",
            "63cd7df981624530b075db80b7804a9f",
            "8e6919c6692c4dd98255acad182b430f",
            "b6391b44be82414c99bde2541b91f3d6",
            "37a3e151e256424fa4845237588bbaa0",
            "4bed91e170684e2ab26eb9cdcb3d20e8",
            "44b6cfc3dc434e18ae9c6f56367743f5",
            "4eb602dcfe7541878ef730de8b0301c8",
            "ee77e82a226a4fb38196c5341fb12e90",
            "399679c3870042dea0d2fcfac456a17c",
            "543c3bd0699d40d4ac7daf4c8b994c07",
            "06435c9510f3443091d48d2cdf1b8b10",
            "4a3a82da55094cdeb2ca07dd6c7b5b49",
            "8daf5b25730e4e73aa7c9d9c6242bd9a",
            "e4fa500e9a484363a3180114e6af101c",
            "206975ed439a4ac1ad0066692a98cacb",
            "8e36bd6128674711b0761c983d226e4d",
            "af20373c8823491d8095e978a699376d",
            "606b7ce18aae49bea146f0b68a4f2af2"
          ]
        },
        "id": "bKJQ951gQrGm",
        "outputId": "10d4c493-ca2c-482b-9af6-8744adc1d2c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eed9683eeaac4417a3870f3e0a485c52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "940d9519138b43c1bef2bdbab6fe7321"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "011dab0ba2fd4842b3dfae0789352807"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc7da3c2fb8c4061ae39d02e156fdc9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7259f5a87d9d40f9a805965b32c227a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f986285831a94b8dac24565aee14521a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee77e82a226a4fb38196c5341fb12e90"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvXEFqAJXSJ4"
      },
      "source": [
        "**GPU Out of Memory!**\n",
        "\n",
        "To reduce the model's memory footprint, we can decrease the memory usage of each parameter through model quantization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Use Mistral 7B Model with Quantization Config**\n"
      ],
      "metadata": {
        "id": "LQ4ofVTgO1ND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "7d0b337d7c88434fa4494293ce103939",
            "87fd41d60c174e74aa66269aeb87fd60",
            "409d4762d5384d6b818488ea70f23965",
            "b7132af9b37c4c18b7bf0e9f145cb855",
            "a37ae65ce4cc4b718efec4c245b94b54",
            "a905a7da48794fe49b9005535a0e9e82",
            "67572f21de3b4fb19a243c9fa9b64d9b",
            "a5942a71eef140f7bac920cbf7ae7e2b",
            "4f7f5732e2234c6289d5daab2b1c528b",
            "d524be80229a468593d821fef1cb0ccb",
            "ed667aabb24a4d43bce4bbfc4fa2758f"
          ]
        },
        "id": "Z2vima76NabB",
        "outputId": "fcdc666b-af32-4c02-b35c-4b89a0b5512a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d0b337d7c88434fa4494293ce103939"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load BitsAndBytes from HuggingFace Transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                     # Use 4-bit quantization (Q = 4 bits)\n",
        "    bnb_4bit_use_double_quant=True,        # Double quantization: quantize the quantization constants to save an additional 0.4 bits per parameter\n",
        "    bnb_4bit_quant_type=\"nf4\",             # Use 4-bit NormalFloat Quantization (optimal for normal weights; enforces w ∈ [-1,1])\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Dequantize to 16-bits before computation (as described in the paper)\n",
        ")\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\",\n",
        "                                             quantization_config=quant_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WleqPvsQcX2x"
      },
      "source": [
        "**3. Loading the Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201,
          "referenced_widgets": [
            "fcc0b23cbcd84eeea9f57ddf24537b9e",
            "c999aa1dff2c490ea7c43d489406181c",
            "54788156d3634b57956b7d9db2513f39",
            "5e58dc91e3344435a081edde7ea09e18",
            "904b461affaf4031aea570c6fcdb310d",
            "7271f74675b349a2beb6e6aaf8ab6cd6",
            "4c579cccae4f44928f2563228bb1ada6",
            "48dbd8c77b3b4f1eae94e1469bd69d8d",
            "b7b93041d4f24f67bef5406dadf1a474",
            "0ceb0aeaa33043d8a77de232c70fb4ce",
            "e48cd7b84c534d1f919044b6ce497f23",
            "5651a5d27fe14a6292f00907496f5dc5",
            "5de43858453d499aad5b57cbf8fe9dd1",
            "f7451c6bbcc3477b8a8eb54b34f06a6d",
            "fe739d524b8d45a4871e675723be8f9f",
            "ed8775242c0c40c2b1d9a7d96c1b4f2c",
            "60f394ed86e344df888c08e365205865",
            "44d7dc188e114ce1879dbaec28d998dd",
            "49a87025d3624e84ad87d8e491718d47",
            "3a0a56e283b64948813b4b7c4519210b",
            "7bf690d447a14d70a47cd74812a78cd7",
            "d7a8f2cd3f104ef38a5d96059ba9edcc",
            "60ad3a8885794ca9b3c6a3e7b4262234",
            "a5e12cb392fd4b488fd056a4b6d5699a",
            "fd45354e7a664e4797b1f145124fdd53",
            "b4d06dd539864dc2839676635fc333c5",
            "c01d2735745d4ffc88e9cca363be985c",
            "daf760ae42084ea6bc7c1820a9afc777",
            "df9d8d132ce24ec6a0d9d0987f5ca28b",
            "54de9e0eacee4c0d855e354cd9c4fd39",
            "3ed6833c118f4b5d8b5c9a49c7ed8ff6",
            "08a8fb22318147f08a0aadc4fa130d33",
            "77c6758c16084c4ab5afbb06fbc46c5d",
            "c7684e44cd92449689c55dc2b0e53ebd",
            "b89c4445015148d7a432361c1e9cb066",
            "4d68fef4f27b4d738a592dfe0d98a7cd",
            "d497214fa23a43dca62b31b68bfb80aa",
            "ed4dde4aac29450e8cebbaf4dfc2f4c4",
            "aef57080261246a19ef3d26985831d53",
            "755a6f02851f47e683b6ae9469827914",
            "3e86e8c0e7c74dda87c998ac2c53334c",
            "c4221dbe20114763ae80e942e1a47ad1",
            "fcb9c019be084052a08c7b1b2a7e66a2",
            "6224b64864e2489f9c74dcd442c0d3ed"
          ]
        },
        "id": "8jLL8njXcaan",
        "outputId": "6fff4600-c13a-43f3-c253-c6df61bc122e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcc0b23cbcd84eeea9f57ddf24537b9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5651a5d27fe14a6292f00907496f5dc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60ad3a8885794ca9b3c6a3e7b4262234"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7684e44cd92449689c55dc2b0e53ebd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\", use_auth_token=os.environ[\"HF_TOKEN\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ensures all sequences in a batch are of equal length.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "4Ry9Vah7RE1d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqvS9ogJr3-e"
      },
      "source": [
        "**3. Testing the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcQiz63Pt23F",
        "outputId": "30b20b85-dbc0-4f89-d34b-aad5a3103e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What do you know about Germany?  Germany is a country in Central Europe. It is\n",
            "bordered to the north by the North Sea, to the northeast by the Baltic Sea, to\n",
            "the east by Poland and the\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "What do you know about Germany?\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "inputs = inputs.to('cuda')\n",
        "\n",
        "output_tokens = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_new_tokens=40,\n",
        "    pad_token_id=model.config.eos_token_id\n",
        ")[0]\n",
        "\n",
        "output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "import textwrap\n",
        "print(textwrap.fill(output, width=80))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU6aS8EZlebB"
      },
      "source": [
        "## Test Model on Dialogue Summarization Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GltHeCTslhfn",
        "outputId": "5dcc2d30-e7c7-448d-ce12-e4e09b131c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summarize the following conversation.\n",
            "\n",
            "### Input: \n",
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "\n",
            "\n",
            "### Summary:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get a dialogue example:\n",
        "dialogue = f\"\"\"\n",
        "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
        "#Person2#: Yes, sir...\n",
        "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
        "#Person2#: Yes, sir. Go ahead.\n",
        "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
        "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
        "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
        "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
        "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
        "#Person2#: This applies to internal and external communications.\n",
        "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
        "#Person2#: Is that all?\n",
        "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "### Input: {dialogue}\n",
        "\n",
        "### Summary:\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YgYAeIVwv4G5",
        "outputId": "530b0b83-7af0-4301-a09d-0044306af610"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Test the model\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "inputs = inputs.to(\"cuda\")\n",
        "\n",
        "output_tokens = model.generate(inputs[\"input_ids\"],\n",
        "                               max_new_tokens=40,\n",
        "                               pad_token_id=model.config.eos_token_id)[0]\n",
        "output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "res = output.replace(prompt,\"\")\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHdEs00jskJp"
      },
      "source": [
        "The model's response is clearly poor; it just repeated new lines and didn’t understand the task. As a result, we can do **fine-tuning** of the model for this task, specifically instruction fine-tuning with LoRA (Low-rank adaptaion)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfwcXJ-zCnt"
      },
      "source": [
        "# Instruction Fine tuning:  LoRa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAYj6DrfIO-B"
      },
      "source": [
        "**1. Loading the Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keFqtbN9tFAQ"
      },
      "source": [
        "This dataset consists of 13,460 dialogues (plus 100 holdout samples for topic generation) along with their manually labeled summaries and topics. The dataset is acquired from HuggingFace under the ID: [knkarthick/dialogsum](https://huggingface.co/datasets/knkarthick/dialogsum).\n",
        "\n",
        "This dataset contains three main features:\n",
        "- **Dialogue**: The text of the dialogue.\n",
        "- **Summary**: A human-written summary of the dialogue.\n",
        "- **Topic**: A human-written topic or one-liner describing the dialogue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lbG27RLLZUmN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "9afd761359674854bfe183c2b1866306",
            "ba4e5aa59920429b964cb35ea37ce64c",
            "64fbc2540ead4b9593446a80f17eae2c",
            "2b7f380dc6104b4082443c02aec7d430",
            "e9c1015eb2e24033a88ee8dc67a37a96",
            "6dc6cc617aec4a7f8bd35993d207b718",
            "4ff576b6a74949a8a79c58a9a913b84f",
            "8fafac081be94c388342ef589b24fbd2",
            "ae89c09d503f4161a7613542a5f2c735",
            "88a6a039f6e7488999b5e89d08f644d1",
            "548efae6d1fd48b8abb7bdd399deafd8",
            "c03a401695494afbb1517a24a8508a48",
            "0da694b1a34c4160898785fecc6680aa",
            "683653edac924c65a391753b096a81f9",
            "1c86144d3d774dafa693cb2080d7f58c",
            "de027a3f766445edb1ae0ecee6aa8417",
            "7c5a9a78fb644abfa1d800eae9f0227f",
            "78c77d7ab00e4778b5e19c062dd10aa6",
            "92d277b614ff40edb2bafa2ffac6ff2f",
            "86554613c7f94dfca4441868d083cc94",
            "7ee02c496d0949b7be22edbe57c130f2",
            "460df5344f1d4803a41f0f543d33fb35",
            "80a2d811708b42128cab5051caceebd4",
            "7806b909a3554e5491e88b6d4e2d53f1",
            "3c50e989b98a443fbcff1d4949264e73",
            "24a59f21e58d44539361a274ce085703",
            "dde23deb484b472db8cdad355ed12e93",
            "b862bf9935fc4ee39594f1081316b520",
            "10710343123d44f3a21aaf58eb8ec6da",
            "d3ec428b3462448ca299113fad64c737",
            "bf1f910ae72a4f64b7c1ad712dfc911d",
            "d41330373b7f475aab387ed2aa6a732c",
            "59ef69767a754c70b755ff7421135ee5",
            "8c245c8d7b05448cbeb5500d13076c7a",
            "15c6cace8bbb488a84e250801907e6b2",
            "d6564c495b624f569350efc15e3f42eb",
            "d655249d2a194632a9ee97d0dff3c641",
            "43c6e465ef7543579060fffa7f51b495",
            "6310b02bb7bc49bd844897e7d1ded89c",
            "e8bccf470c304374953b30dc1f51ff49",
            "4bdca6dafdd04e47a0c6347d6b873e0f",
            "a48a84af382946d68ad8238658462b5a",
            "82a0715302ae40d79f59c2da5bd11d59",
            "46bea6fa823f4d53bce4a1a1430590e0",
            "05223dcbab9f4d7d95067c65f5e8f433",
            "f1ac36b4c2064e96898edefa9dfa19b1",
            "bcd2b7cec5b347359a3ccc29d68be6c1",
            "f349a3cf4a5f4c08ad130872db729cf0",
            "ac7f0904aee1488fac752bf28e2ad4d0",
            "31b8478780bb4f2bb898554572b5bdb7",
            "b507166f727e4fa69b0e1b3edec03e9e",
            "d0b36f7a6b45471e95cb1db1f45ff6bf",
            "934d4279b57245c59976764bd4e37d30",
            "c1a9980c25764c78b95d71af62a13781",
            "b824ca0ab6dc4d968d662e5bb3550160",
            "0915f862b592433f91e0a3b1a4516368",
            "57c6ce7395e145d5967ea66906417e35",
            "6f1d4a6b72f843a6b979bf526cd7922a",
            "33885d1b0688462f8247d05f87524632",
            "bfad20ec5de74c23a3947b81a13b9119",
            "5fd60a37b3954b4f9eb10f3cd497f4a4",
            "7ba65047f86347b69fff93472e3d1065",
            "a611d17bf92640399da47c703066ae2c",
            "6e25bbff0b6849e8b3c73cdc1802cbca",
            "04a8909666dd4a7fb553916b6b89f332",
            "a41c58f45bbe4c729bdbe7b5609a148a",
            "35a12d29438c4d6c8abd714de496dd49",
            "639c3bfbf34444009794fc073b0617df",
            "18a3183abdab47eca91b2b51a24d87e5",
            "f9405adf449343028d78cee03472920a",
            "21b286bd5ef7494caa0463dca99cfbf4",
            "23b17bfd9b3749d795192bb9df6e2402",
            "6e92c125ad314e4c899ac55550f0b841",
            "430f81059bfa4417963743e33491c63f",
            "2146354edd004de99ff80d9b07011db0",
            "8f02a954189443208d75e83bbf871b93",
            "e84e9287238c4d849a5b6a7b03b35cb9"
          ]
        },
        "outputId": "2155913f-e05a-4762-9546-2b9e8f7403fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9afd761359674854bfe183c2b1866306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c03a401695494afbb1517a24a8508a48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation.csv:   0%|          | 0.00/442k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80a2d811708b42128cab5051caceebd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.csv:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c245c8d7b05448cbeb5500d13076c7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05223dcbab9f4d7d95067c65f5e8f433"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0915f862b592433f91e0a3b1a4516368"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35a12d29438c4d6c8abd714de496dd49"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"knkarthick/dialogsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFtqbVg918Eh",
        "outputId": "319ba929-9063-49de-cfec-03f61c72410f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anBQejLB2ELN"
      },
      "source": [
        "The dataset is divided into training, validation, and test sets, with **12,460**, **500**, and **1,500** examples, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP5ponNy2IIS",
        "outputId": "581f0c8e-bdd5-4be7-ce14-794e86f69e8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'train_0',\n",
              " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
              " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
              " 'topic': 'get a check-up'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzO7rxTJIYwD"
      },
      "source": [
        "**2. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2jPq4D-IifN"
      },
      "source": [
        "Given a dialogue `D` and its human-written summary `S`, we will use the following prompt format:\n",
        "```\n",
        "### Instruction:\n",
        "Summarize the following conversation.\n",
        "\n",
        "### Input:\n",
        "{D}\n",
        "\n",
        "### Summary:\n",
        "{S}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "z-QDPd1JlIfV"
      },
      "outputs": [],
      "source": [
        "def format_instruction(dialogue, summary):\n",
        "    return f\"\"\"### Instruction:\n",
        "Summarize the following conversation.\n",
        "\n",
        "### Input:\n",
        "{dialogue.strip()}\n",
        "\n",
        "### Summary:\n",
        "{summary.strip()}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sAJqa7lRkg4R"
      },
      "outputs": [],
      "source": [
        "def convert_to_instruction_format(data_point):\n",
        "    return {\n",
        "        \"text\": format_instruction(data_point[\"dialogue\"], data_point[\"summary\"])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h5Hj9clPmvKY"
      },
      "outputs": [],
      "source": [
        "def process_dataset(data):\n",
        "    return data.map(\n",
        "        convert_to_instruction_format\n",
        "        ).remove_columns(['id', 'topic', 'dialogue', 'summary']) #removing unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8127467a07684b1fbaa1c3f1cd1c7890",
            "3838648b61264e11bf80d22a1368c443",
            "1c24085abd434659a9fea4f71bd768da",
            "57ddd710441b46239364bb131adeb9c8",
            "bfb3a4b85f1242fb8e5af84a8f570cb4",
            "b756be0d44a74c2b9c55e58c46cf8188",
            "9f43be0a952146aeb74e7cdb3eb5024a",
            "cf02be0db5064e8295c67dcc670beda7",
            "e3ef51e9605148f79a105a9baaf2dbe4",
            "636cd2ff0bd34ff3859ac7a51c7ad506",
            "51a8df1325d5431495577204f0e14d21",
            "8e32af29a4d640cb9e5e2c7898a8f31b",
            "86f6d02d3ff940f8a621ad4dd9f3d258",
            "7be5ac12de6949d2a1a269ab36b87075",
            "1e886dfe7bf5476d98b598a8d704cac2",
            "40a9921b41664bf299e2596d01919f33",
            "66e6a37722e64c999a6f833a37a0db1e",
            "9af2a9a7d7a8459c847732569e169c41",
            "c3a618b8f76a4c159522bfce14cc9ff0",
            "6277bfac79034f4589f0cdc282b62ea4",
            "28ea5434c2674ef981e46cfebb291022",
            "954ee741c0c640daa761002eb4c152b6"
          ]
        },
        "id": "LEyJFjPbKW2l",
        "outputId": "8cd98fac-3262-43b3-c99f-afbad9d4b2f7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8127467a07684b1fbaa1c3f1cd1c7890"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e32af29a4d640cb9e5e2c7898a8f31b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Take a random sample from the training and validation sets and apply process_dataset to them.\n",
        "train_data = process_dataset(\n",
        "    dataset[\"train\"].shuffle(seed=42).select([i for i in range(500)])\n",
        "    )\n",
        "validation_data  = process_dataset(\n",
        "    dataset[\"validation\"].shuffle(seed=42).select([i for i in range(50)])\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ6biD9fipyh",
        "outputId": "2ff595c9-529f-4def-e683-7c01087fca68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkGfRPpqkrRN",
        "outputId": "fefd0736-983b-4a47-ec7b-e6e489c55fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Summarize the following conversation.\n",
            "\n",
            "### Input:\n",
            "#Person1#: Hello, Anna speaking!\n",
            "#Person2#: Hey, Anna, this is Jason.\n",
            "#Person1#: Jason, where have you been hiding lately? You know it's been a long time since your last call. Have you been good?\n",
            "#Person2#: Yes. How are you, Anna?\n",
            "#Person1#: I am fine. What have you been doing?\n",
            "#Person2#: Working. I've been really busy these days. I got a promotion.\n",
            "#Person1#: That's great, congratulations!\n",
            "#Person2#: Thanks. I am feeling pretty good about myself too. You know, bigger office, a raise and even an assistant.\n",
            "#Person1#: That's good. So I guess I'll have to make an appointment to see you.\n",
            "#Person2#: You are kidding.\n",
            "#Person1#: How long have you been working there?\n",
            "#Person2#: A bit over two years. This is a fast-moving company, and seniority isn't the only factor in deciding promotions.\n",
            "#Person1#: How do you like your new boss?\n",
            "#Person2#: She is very nice and open-minded.\n",
            "#Person1#: Much better than the last one, huh?\n",
            "#Person2#: Yeah. He was a real slave driver. He probably would have loved it if we were robots.\n",
            "#Person1#: Forget about him. Come over to my house tonight. Let's get drunk.\n",
            "#Person2#: Good. Tonight 8 o'clock.\n",
            "#Person1#: 8 it is. See you then.\n",
            "#Person2#: Bye.\n",
            "\n",
            "### Summary:\n",
            "Jason hasn't called Anna for a long time. He calls her to tell her he got a promotion and he feels good about it. Anna invites him to come over to her house tonight to get drunk.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(train_data['text'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZJbpXXc7U0Q"
      },
      "source": [
        "**3. PEFT Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7K7pkdSpBOz",
        "outputId": "651da858-0be0-402c-dd49-0d0ba7c14718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32768, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): MistralRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H26PtR-Z9v8S"
      },
      "source": [
        "Looking at the model architecture, we could apply LoRA layers to all layers in the model. However, by default, it is sufficient to add LoRA layers only to the self-attention layers, specifically the components: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]. These are the key components responsible for the attention mechanism, and fine-tuning them with LoRA can significantly enhance the model’s performance without the need for extensive modifications across the entire architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a9EUEDAl0ss3"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                                                       # The rank (dimensions) of the LoRA matrices A and B\n",
        "    lora_alpha=64,                                              # Scales the product of matrices AB [W_new = W_old + (A * B) * α]\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],    # Apply LoRA to the attention matrices\n",
        "    lora_dropout=0.1,                                           # Dropout rate to reduce overfitting\n",
        "    bias=\"none\",                                                # Do not train the bias parameter\n",
        "    task_type=\"CAUSAL_LM\"                                       # Task type for autoregressive text generation\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzCg8RJg7csh"
      },
      "source": [
        "**4. Fine-Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jq0nX33BmfaC"
      },
      "outputs": [],
      "source": [
        "# Configure the training hyperparameters using the `SFTConfig` from HuggingFace.\n",
        "from trl import SFTConfig\n",
        "\n",
        "training_arguments = SFTConfig(\n",
        "    fp16=True,                           # Use 16-bit precision for training computations (optimizer states, gradients)\n",
        "    dataset_text_field=\"text\",           # Specify the text field in the dataset for training\n",
        "    max_seq_length=1024,                 # Set the maximum sequence length for the training data\n",
        "\n",
        "    # Batch-related parameters\n",
        "    per_device_train_batch_size=8,       # Batch size per device during training\n",
        "\n",
        "    # Optimizer-related parameters\n",
        "    optim=\"paged_adamw_32bit\",           # Use the paged AdamW optimizer, optimized for 32-bit GPUs\n",
        "    learning_rate=1e-4,                  # Set the learning rate for training\n",
        "\n",
        "    # Epochs and saving configuration\n",
        "    num_train_epochs=2,                  # Number of training epochs (more epochs generally lead to better results)\n",
        "    save_strategy=\"epoch\",               # Save the model after each epoch\n",
        "    output_dir=\"./epoch-finetuned\",      # Directory to save the fine-tuned model\n",
        "\n",
        "    # Validation-related parameters\n",
        "    eval_strategy=\"steps\",               # Evaluation strategy, performed at specified steps\n",
        "    eval_steps=0.2,                      # Evaluate after 20% of the training steps\n",
        "\n",
        "    # Logging-related parameters\n",
        "    report_to=\"none\",                    # Disable reporting to external tools\n",
        "    logging_dir=\"./logs\",                # Directory to save the training logs\n",
        "    logging_steps=20,                    # Number of steps between each log entry\n",
        "    seed=42,                             # Set a random seed for reproducibility\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311,
          "referenced_widgets": [
            "119fd277add3421d89a1a85602fa8e86",
            "41947c8ae0d843d7bba59e3e09eccfba",
            "51a0f93b224d4ad6a7e8f3630afe493a",
            "8d8d4db102294f4493cd447c11353bfa",
            "dbfbaec61f5a455ba17087450e3607e2",
            "d0bdcd598c1c417ba74efbec7af440a2",
            "90cc6d63c18e4c82a742d468c8dc8463",
            "ee7ac0f01f1b4b4fabe4bb63abe89d0e",
            "cb4a4ded70ae43a4a8bb96d2ad69a7cf",
            "8f73fe9509734084970bd9a1beb7c751",
            "c3604da4189e4d86b2145bca6c683b91",
            "f6cfde72891d4773a22a32a52a4e573b",
            "917709088ffa43639188e1b20288ee1d",
            "8c9d13b1a24541c39116cfda54120ee7",
            "cfe69f42d8a44864b31b999093702576",
            "528df2bffda248a68210d77424474829",
            "02d4e7cf3997467ea974b3b8e32fef48",
            "8cb76efe5dd742e081cdd4a61dddac30",
            "98ebea3e186444a99e08d6a9b4f95eb5",
            "519408d134524cebb7ea3b953861f828",
            "659e782f68f9423bb74ee64f68bf9d99",
            "6caf03ec4b53488eba7531ef9025e87f",
            "f5e7ab0122d8469cbae848ca529f7513",
            "9ae66e173e6b47fc917e25927bcd99f0",
            "035a67afa0cd4a4cb2330d11c3170120",
            "91ec9a47f81a41bcb0a53ca7036090b0",
            "e6977173ae9c4ffea59bf6e061998d74",
            "85277a80d4134d0895f61564f8e64fc8",
            "027f2b44674f487aa928f246fc9e9f45",
            "67ed550f204048b497575cb13bfd915f",
            "b8aeebe9cc9d43189fee2e5ef602b238",
            "dd0f65e5817e42598b3af1bcfb89d129",
            "5751604214454e2bb1aa8d377ec9f9b7",
            "d2a7d6bb46de41b0b1c7dc479c8ad89a",
            "5264f97854604f2e8de15fac4a9e04b7",
            "9113c17f5449482eb1a0d6789e9c66c1",
            "ab88226fec574aaf9f1d6ba7a4287786",
            "e6dfb367ace54d499b8d9f76dbbba5e8",
            "10be60a35ce9411596402188524954fb",
            "fd36dc0d5980449c974c527d29651ee2",
            "89cbff12b3ca44759e187ac98ab00046",
            "db6b78d8f7e740b99c9ae666b6583fb5",
            "9c1fa9df142a423b8089e749fd46af27",
            "1d89e9bd5ad04111baa48434fe59905b",
            "c4989a361dd24cee81017f9e7ab8c4a8",
            "42880b5f656f49f3ac89740c0cfe72ae",
            "5af1856ac6e044a9abf963465f81cc83",
            "bb70f1201e894f94b0c8bffb65f1f104",
            "07f9c5ef69e04c82998c7f8e892a0b51",
            "d5d00fe66a3744a59f33c75e32f2200f",
            "e720d47cee0b4d76940950ab1bc3f66a",
            "711e927b04b546b5a984f22c2c1b43fd",
            "153bec5117de4292a0346e2c48b5abf8",
            "7f68f5d40c3d4c9fa36e7e1356459679",
            "efeeb55070a1486aad0bee18f2de6fc6",
            "d0c20bce9098417b88f3464bda8926f6",
            "52f8ad41da88487ab345035a8cbb056a",
            "5b48fead39ab42d884b8673da617f83a",
            "a32b0d205fec4319906b760308cfcb98",
            "bee41fd13d4742e09bbde49a62ef9d91",
            "89c9a86ba751492b818a265f9c65c79b",
            "644a51abe69947c7be0b460c9ad9573b",
            "a06bea8348a742fd96cbf8deb13befee",
            "70bd853d534943fcbc52e3d1467cb312",
            "974372fdcbec4c03bb86c4fdea3e63c8",
            "4b233c75f9da43dbac69ac09b1ea7593",
            "e458e633f29e4a578045aad92830883b",
            "bcfb3f23599640a9b3dba2700c0aa2ce",
            "461fdacda93a4c6da304496b2d6e0187",
            "8dd26157db8d45c78b7c13eff946eb4c",
            "395df210276f4ab4b76438601bbc1f6d",
            "eb3dc3afd21247eebc1d9cdee8460cad",
            "d536b300b44a4242891f8c4216ac6f2c",
            "3d29da0d8d984efa9594535020c39656",
            "3b2f70574fb24b759ca3b1239e6ca1d5",
            "2f1335e379b3416f9e0448c01386b603",
            "f9959ccaaf5c4707a6bff8c5dc79fea0",
            "6644c4d93bf8436bb95b5580ba83a581",
            "a2dcc931bcd74a9c8f2d992e650110c8",
            "480d6deb7a5b481d97a541c2616fcf5a",
            "955b61ecb6294fc797c49abf3ffac44c",
            "8f459b2681d74437b20f1a8d6f38fb5b",
            "91095e9e985f42c5bc4631ffff73f93e",
            "5ad6faca8b4a49db876f6cd312e081a5",
            "f422447502954ef7a6cbaf0438089e57",
            "ce3711cc33f0496188b9927c304f1136",
            "4e482222fd5d4a429b0d01e6fb609b8f",
            "45265313d2724aa2a990b67de99b2e89"
          ]
        },
        "id": "t5hwFT0BvBwa",
        "outputId": "d4d16f00-f81f-4e17-a80f-b26c40f5d737"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "119fd277add3421d89a1a85602fa8e86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6cfde72891d4773a22a32a52a4e573b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5e7ab0122d8469cbae848ca529f7513"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2a7d6bb46de41b0b1c7dc479c8ad89a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting eval dataset to ChatML:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4989a361dd24cee81017f9e7ab8c4a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding EOS to eval dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0c20bce9098417b88f3464bda8926f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e458e633f29e4a578045aad92830883b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6644c4d93bf8436bb95b5580ba83a581"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "# Import the SFTTrainer from HuggingFace TRL library\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "\n",
        "    model=model,\n",
        "    # tokenizer=tokenizer,\n",
        "\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=validation_data,\n",
        "\n",
        "    peft_config=lora_config,\n",
        "\n",
        "    args=training_arguments,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "DiqXVQpBOIpQ",
        "outputId": "e4ee3619-ac9d-44d2-88d6-fb18bd6d13f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 25:09, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.354000</td>\n",
              "      <td>1.299895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.286200</td>\n",
              "      <td>1.281364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.271000</td>\n",
              "      <td>1.281662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.138900</td>\n",
              "      <td>1.280206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=126, training_loss=1.2344436456286718, metrics={'train_runtime': 1521.4762, 'train_samples_per_second': 0.657, 'train_steps_per_second': 0.083, 'total_flos': 2.0783094026305536e+16, 'train_loss': 1.2344436456286718})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Start the training process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZinIOFxl9fjL",
        "outputId": "f651385b-6680-414a-ada9-60c6f36393ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 57M\n",
            "-rw-r--r-- 1 root root  808 May  7 09:43 adapter_config.json\n",
            "-rw-r--r-- 1 root root  53M May  7 09:43 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root 5.0K May  7 09:43 README.md\n",
            "-rw-r--r-- 1 root root  437 May  7 09:43 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 134K May  7 09:43 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 3.6M May  7 09:43 tokenizer.json\n",
            "-rw-r--r-- 1 root root 574K May  7 09:43 tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "peft_model_path = \"./fine-tuned-mistral\"\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)\n",
        "\n",
        "tokenizer.save_pretrained(peft_model_path)\n",
        "\n",
        "!ls -lh {peft_model_path}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhsbhJmKQm2Z"
      },
      "source": [
        "**5. Test Tuned model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "fee9211d6e204b7e8ee084a2d336f11d",
            "a862465102584bffbb73288ac390e24f",
            "c642fb05b39f487ba5ea4d485c7b2623",
            "33506b63e9b9408f8f50ed77b4b878f3",
            "e9ad0a15b9d34a1cafe37b5002fd4c8c",
            "de166a5f6d21414691e463d80d3b0007",
            "6c989a602a6f45f8bb2c396d1be90aff",
            "3294c39726914b0e8bfea32dafd31dd7",
            "af77b5ed05fd4bac9003be5151c0a6ab",
            "6c894b519b004e8880c9eeb9c87ecbf1",
            "db203939531c48f7add2249427a8dc65",
            "1eddda83266842879c2b4013eabe8bb5",
            "930c3646cfa4473ea21be0da5385ec0e",
            "767de16f40dc40728d1f13b661f2a739",
            "ab38b96395c1413392d8b4051a1aa3b3",
            "0a22ccdaee444a8b91b7484c177e823a",
            "d09e6594da2a4cfbb99b46623546b947",
            "74614e7c781d40dabff8aaffb93c22ca",
            "755b9621d5074153883b0743b75e090b",
            "4e9378ca6142463fa6862e7ee2151bf1",
            "0c541bf5d88f461e9253c3e7dbc9ba33",
            "dfd93bf5b8854960a53cb4812c927e1b"
          ]
        },
        "id": "RAXQGIGABYuz",
        "outputId": "5356a571-1860-4983-8bd7-d2f0ae9c06dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fee9211d6e204b7e8ee084a2d336f11d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1eddda83266842879c2b4013eabe8bb5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "peft_model_path = \"./fine-tuned-mistral\"\n",
        "tuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    peft_model_path,\n",
        "    quantization_config=quant_config\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_path)\n",
        "\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "tuned_model.config.use_cache = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KMSOP2JbJxkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c441c2c-d888-4c6e-bbd6-1091f94311cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summarize the following conversation.\n",
            "\n",
            "### Input: \n",
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "\n",
            "\n",
            "### Summary:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get a dialogue example:\n",
        "dialogue = f\"\"\"\n",
        "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
        "#Person2#: Yes, sir...\n",
        "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
        "#Person2#: Yes, sir. Go ahead.\n",
        "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
        "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
        "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
        "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
        "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
        "#Person2#: This applies to internal and external communications.\n",
        "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
        "#Person2#: Is that all?\n",
        "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
        "\"\"\"\n",
        "\n",
        "# Build an instruction prompt to the model\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "### Input: {dialogue}\n",
        "\n",
        "### Summary:\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nNdghmyCJ6vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc43b00-a116-406b-e64b-8ec3d1b0cdf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINED MODEL GENERATED TEXT :\n",
            "#Person1# asks Ms. Dawson to take a dictation for him. #Person1# tells Ms.\n",
            "Dawson to write a memo to all employees, prohibiting the use of Instant Message\n",
            "programs by employees during working hours.\n"
          ]
        }
      ],
      "source": [
        "# Test fine-tuned model\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "inputs = inputs.to(\"cuda\")\n",
        "\n",
        "output_tokens = tuned_model.generate(inputs[\"input_ids\"],\n",
        "                               max_new_tokens=100,\n",
        "                               pad_token_id=model.config.eos_token_id)[0]\n",
        "output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "res = output.replace(prompt,\"\")\n",
        "\n",
        "import textwrap\n",
        "print('TRAINED MODEL GENERATED TEXT :')\n",
        "print(textwrap.fill(res, width=80))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMAuqy1-ZfTI"
      },
      "source": [
        "The model now provides significantly improved responses! Optional to further fine-tune, so its performance can continue to enhance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
